{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORvwib-YSR-p"
      },
      "source": [
        "CIFAR10 - DenseNet solution. AdaptaciÃ³n de https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/applications/densenet.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YRXPjE-Sf1a"
      },
      "source": [
        "Install Keras just in case..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhsFJNHpSSKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adda23f5-b6ba-4f59-ea61-7beb481f0824"
      },
      "source": [
        "!pip3 install keras\n",
        "!pip3 install keras_applications"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.5.2)\n",
            "Requirement already satisfied: keras_applications in /usr/local/lib/python3.7/dist-packages (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications) (1.19.5)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications) (1.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGX8RGwHSSTr"
      },
      "source": [
        "Imports..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8qeBuPiSSdc"
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "\n",
        "import warnings\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.pooling import MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Input, Conv2D, Add, ZeroPadding2D, Dense, Dropout, Activation, Reshape, Conv2DTranspose, UpSampling2D, MaxPooling2D, AveragePooling2D, GlobalMaxPooling2D, concatenate\n",
        "from keras.layers.normalization import BatchNormalization as  BN\n",
        "from keras.layers.merge import add\n",
        "from keras.model import Model\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras.regularizers import l2\n",
        "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from keras.applications.imagenet_utils import preprocess_input as _preprocess_input\n",
        "import keras.backend as K\n",
        "from keras import utils\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M9uBGifSSqd"
      },
      "source": [
        "Define batch size, number of epochs and number of classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaWFWm8mSTBj"
      },
      "source": [
        "batch_size = 100\n",
        "num_classes = 10\n",
        "epochs = 75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuM-ZML22Cn9",
        "outputId": "00e38689-94cd-4ab8-ae20-3ea8f3d735c5"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0CgF6jTejyx"
      },
      "source": [
        "## Data Augmentation with an ImageGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.3,\n",
        "    rotation_range=45,\n",
        "    vertical_flip=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdaMLtdhtd5j"
      },
      "source": [
        "Movidas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap_0BFfhRKEC"
      },
      "source": [
        "def __conv_block(ip, nb_filter, bottleneck=False, dropout_rate=None,\n",
        "                 weight_decay=1e-4, block_prefix=None):\n",
        "    '''\n",
        "    Adds a convolution layer (with batch normalization and relu),\n",
        "    and optionally a bottleneck layer.\n",
        "    # Arguments\n",
        "        ip: Input tensor\n",
        "        nb_filter: integer, the dimensionality of the output space\n",
        "            (i.e. the number output of filters in the convolution)\n",
        "        bottleneck: if True, adds a bottleneck convolution block\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay factor\n",
        "        block_prefix: str, for unique layer naming\n",
        "     # Input shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, channels, rows, cols)` if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, rows, cols, channels)` if data_format='channels_last'.\n",
        "    # Output shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n",
        "        `rows` and `cols` values might have changed due to stride.\n",
        "    # Returns\n",
        "        output tensor of block\n",
        "    '''\n",
        "    with K.name_scope('ConvBlock'):\n",
        "        concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5,\n",
        "                               name=name_or_none(block_prefix, '_bn'))(ip)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        if bottleneck:\n",
        "            inter_channel = nb_filter * 4\n",
        "\n",
        "            x = Conv2D(inter_channel, (1, 1), kernel_initializer='he_normal',\n",
        "                       padding='same', use_bias=False,\n",
        "                       kernel_regularizer=l2(weight_decay),\n",
        "                       name=name_or_none(block_prefix, '_bottleneck_conv2D'))(x)\n",
        "            x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5,\n",
        "                                   name=name_or_none(block_prefix, '_bottleneck_bn'))(x)\n",
        "            x = Activation('relu')(x)\n",
        "\n",
        "        x = Conv2D(nb_filter, (3, 3), kernel_initializer='he_normal', padding='same',\n",
        "                   use_bias=False, name=name_or_none(block_prefix, '_conv2D'))(x)\n",
        "        if dropout_rate:\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwkmg_n-Rssm"
      },
      "source": [
        "def __dense_block(x, nb_layers, nb_filter, growth_rate, bottleneck=False,\n",
        "                  dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True,\n",
        "                  return_concat_list=False, block_prefix=None):\n",
        "    '''\n",
        "    Build a dense_block where the output of each conv_block is fed\n",
        "    to subsequent ones\n",
        "    # Arguments\n",
        "        x: input keras tensor\n",
        "        nb_layers: the number of conv_blocks to append to the model\n",
        "        nb_filter: integer, the dimensionality of the output space\n",
        "            (i.e. the number output of filters in the convolution)\n",
        "        growth_rate: growth rate of the dense block\n",
        "        bottleneck: if True, adds a bottleneck convolution block to\n",
        "            each conv_block\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay factor\n",
        "        grow_nb_filters: if True, allows number of filters to grow\n",
        "        return_concat_list: set to True to return the list of\n",
        "            feature maps along with the actual output\n",
        "        block_prefix: str, for block unique naming\n",
        "    # Return\n",
        "        If return_concat_list is True, returns a list of the output\n",
        "        keras tensor, the number of filters and a list of all the\n",
        "        dense blocks added to the keras tensor\n",
        "        If return_concat_list is False, returns a list of the output\n",
        "        keras tensor and the number of filters\n",
        "    '''\n",
        "    with K.name_scope('DenseBlock'):\n",
        "        concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "        x_list = [x]\n",
        "\n",
        "        for i in range(nb_layers):\n",
        "            cb = __conv_block(x, growth_rate, bottleneck, dropout_rate, weight_decay,\n",
        "                              block_prefix=name_or_none(block_prefix, '_%i' % i))\n",
        "            x_list.append(cb)\n",
        "\n",
        "            x = concatenate([x, cb], axis=concat_axis)\n",
        "\n",
        "            if grow_nb_filters:\n",
        "                nb_filter += growth_rate\n",
        "\n",
        "        if return_concat_list:\n",
        "            return x, nb_filter, x_list\n",
        "        else:\n",
        "            return x, nb_filter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt9-YGpZRtoj"
      },
      "source": [
        "def __transition_block(ip, nb_filter, compression=1.0, weight_decay=1e-4,\n",
        "                       block_prefix=None, transition_pooling='max'):\n",
        "    '''\n",
        "    Adds a pointwise convolution layer (with batch normalization and relu),\n",
        "    and an average pooling layer. The number of output convolution filters\n",
        "    can be reduced by appropriately reducing the compression parameter.\n",
        "    # Arguments\n",
        "        ip: input keras tensor\n",
        "        nb_filter: integer, the dimensionality of the output space\n",
        "            (i.e. the number output of filters in the convolution)\n",
        "        compression: calculated as 1 - reduction. Reduces the number\n",
        "            of feature maps in the transition block.\n",
        "        weight_decay: weight decay factor\n",
        "        block_prefix: str, for block unique naming\n",
        "    # Input shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, channels, rows, cols)` if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, rows, cols, channels)` if data_format='channels_last'.\n",
        "    # Output shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, nb_filter * compression, rows / 2, cols / 2)`\n",
        "        if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, rows / 2, cols / 2, nb_filter * compression)`\n",
        "        if data_format='channels_last'.\n",
        "    # Returns\n",
        "        a keras tensor\n",
        "    '''\n",
        "    with K.name_scope('Transition'):\n",
        "        concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5,\n",
        "                               name=name_or_none(block_prefix, '_bn'))(ip)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Conv2D(int(nb_filter * compression), (1, 1), kernel_initializer='he_normal',\n",
        "                   padding='same', use_bias=False, kernel_regularizer=l2(weight_decay),\n",
        "                   name=name_or_none(block_prefix, '_conv2D'))(x)\n",
        "        if transition_pooling == 'avg':\n",
        "            x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "        elif transition_pooling == 'max':\n",
        "            x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pbEvFvxf5z_"
      },
      "source": [
        "def __transition_up_block(ip, nb_filters, type='deconv', weight_decay=1E-4,\n",
        "                          block_prefix=None):\n",
        "    '''Adds an upsampling block. Upsampling operation relies on the the type parameter.\n",
        "    # Arguments\n",
        "        ip: input keras tensor\n",
        "        nb_filters: integer, the dimensionality of the output space\n",
        "            (i.e. the number output of filters in the convolution)\n",
        "        type: can be 'upsampling', 'subpixel', 'deconv'. Determines\n",
        "            type of upsampling performed\n",
        "        weight_decay: weight decay factor\n",
        "        block_prefix: str, for block unique naming\n",
        "    # Input shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, channels, rows, cols)` if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, rows, cols, channels)` if data_format='channels_last'.\n",
        "    # Output shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, nb_filter, rows * 2, cols * 2)` if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, rows * 2, cols * 2, nb_filter)` if data_format='channels_last'.\n",
        "    # Returns\n",
        "        a keras tensor\n",
        "    '''\n",
        "    with K.name_scope('TransitionUp'):\n",
        "\n",
        "        if type == 'upsampling':\n",
        "            x = UpSampling2D(name=name_or_none(block_prefix, '_upsampling'))(ip)\n",
        "        elif type == 'subpixel':\n",
        "            x = Conv2D(nb_filters, (3, 3), activation='relu', padding='same',\n",
        "                       kernel_regularizer=l2(weight_decay), use_bias=False,\n",
        "                       kernel_initializer='he_normal',\n",
        "                       name=name_or_none(block_prefix, '_conv2D'))(ip)\n",
        "            x = SubPixelUpscaling(scale_factor=2,\n",
        "                                  name=name_or_none(block_prefix, '_subpixel'))(x)\n",
        "            x = Conv2D(nb_filters, (3, 3), activation='relu', padding='same',\n",
        "                       kernel_regularizer=l2(weight_decay), use_bias=False,\n",
        "                       kernel_initializer='he_normal',\n",
        "                       name=name_or_none(block_prefix, '_conv2D'))(x)\n",
        "        else:\n",
        "            x = Conv2DTranspose(nb_filters, (3, 3), activation='relu', padding='same',\n",
        "                                strides=(2, 2), kernel_initializer='he_normal',\n",
        "                                kernel_regularizer=l2(weight_decay),\n",
        "                                name=name_or_none(block_prefix, '_conv2DT'))(ip)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9L_jJ0ff5_s"
      },
      "source": [
        "def __create_dense_net(nb_classes, img_input, include_top, depth=40, nb_dense_block=3,\n",
        "                       growth_rate=12, nb_filter=-1, nb_layers_per_block=-1,\n",
        "                       bottleneck=False, reduction=0.0, dropout_rate=None,\n",
        "                       weight_decay=1e-4, subsample_initial_block=False, pooling=None,\n",
        "                       activation='softmax', transition_pooling='avg'):\n",
        "    ''' Build the DenseNet model\n",
        "    # Arguments\n",
        "        nb_classes: number of classes\n",
        "        img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n",
        "        include_top: flag to include the final Dense layer\n",
        "        depth: number or layers\n",
        "        nb_dense_block: number of dense blocks to add to end (generally = 3)\n",
        "        growth_rate: number of filters to add per dense block\n",
        "        nb_filter: initial number of filters. Default -1 indicates initial number\n",
        "            of filters is 2 * growth_rate\n",
        "        nb_layers_per_block: number of layers in each dense block.\n",
        "                Can be a -1, positive integer or a list.\n",
        "                If -1, calculates nb_layer_per_block from the depth of the network.\n",
        "                If positive integer, a set number of layers per dense block.\n",
        "                If list, nb_layer is used as provided. Note that list size must\n",
        "                be (nb_dense_block + 1)\n",
        "        bottleneck: add bottleneck blocks\n",
        "        reduction: reduction factor of transition blocks. Note : reduction value is\n",
        "            inverted to compute compression\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay rate\n",
        "        subsample_initial_block: Changes model type to suit different datasets.\n",
        "            Should be set to True for ImageNet, and False for CIFAR datasets.\n",
        "            When set to True, the initial convolution will be strided and\n",
        "            adds a MaxPooling2D before the initial dense block.\n",
        "        pooling: Optional pooling mode for feature extraction\n",
        "            when `include_top` is `False`.\n",
        "            - `None` means that the output of the model\n",
        "                will be the 4D tensor output of the\n",
        "                last convolutional layer.\n",
        "            - `avg` means that global average pooling\n",
        "                will be applied to the output of the\n",
        "                last convolutional layer, and thus\n",
        "                the output of the model will be a\n",
        "                2D tensor.\n",
        "            - `max` means that global max pooling will\n",
        "                be applied.\n",
        "        activation: Type of activation at the top layer. Can be one of 'softmax' or\n",
        "            'sigmoid'. Note that if sigmoid is used, classes must be 1.\n",
        "        transition_pooling: `avg` for avg pooling (default), `max` for max pooling,\n",
        "            None for no pooling during scale transition blocks. Please note that this\n",
        "            default differs from the DenseNetFCN paper in accordance with the DenseNet\n",
        "            paper.\n",
        "    # Returns\n",
        "        a keras tensor\n",
        "    # Raises\n",
        "        ValueError: in case of invalid argument for `reduction`\n",
        "            or `nb_dense_block`\n",
        "    '''\n",
        "    with K.name_scope('DenseNet'):\n",
        "        concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "        if reduction != 0.0:\n",
        "            if not (reduction <= 1.0 and reduction > 0.0):\n",
        "                raise ValueError('`reduction` value must lie between 0.0 and 1.0')\n",
        "\n",
        "        # layers in each dense block\n",
        "        if type(nb_layers_per_block) is list or type(nb_layers_per_block) is tuple:\n",
        "            nb_layers = list(nb_layers_per_block)  # Convert tuple to list\n",
        "\n",
        "            if len(nb_layers) != nb_dense_block:\n",
        "                raise ValueError('If `nb_dense_block` is a list, its length must match '\n",
        "                                 'the number of layers provided by `nb_layers`.')\n",
        "\n",
        "            final_nb_layer = nb_layers[-1]\n",
        "            nb_layers = nb_layers[:-1]\n",
        "        else:\n",
        "            if nb_layers_per_block == -1:\n",
        "                assert (depth - 4) % 3 == 0, ('Depth must be 3 N + 4 '\n",
        "                                              'if nb_layers_per_block == -1')\n",
        "                count = int((depth - 4) / 3)\n",
        "\n",
        "                if bottleneck:\n",
        "                    count = count // 2\n",
        "\n",
        "                nb_layers = [count for _ in range(nb_dense_block)]\n",
        "                final_nb_layer = count\n",
        "            else:\n",
        "                final_nb_layer = nb_layers_per_block\n",
        "                nb_layers = [nb_layers_per_block] * nb_dense_block\n",
        "\n",
        "        # compute initial nb_filter if -1, else accept users initial nb_filter\n",
        "        if nb_filter <= 0:\n",
        "            nb_filter = 2 * growth_rate\n",
        "\n",
        "        # compute compression factor\n",
        "        compression = 1.0 - reduction\n",
        "\n",
        "        # Initial convolution\n",
        "        if subsample_initial_block:\n",
        "            initial_kernel = (7, 7)\n",
        "            initial_strides = (2, 2)\n",
        "        else:\n",
        "            initial_kernel = (3, 3)\n",
        "            initial_strides = (1, 1)\n",
        "\n",
        "        x = Conv2D(nb_filter, initial_kernel, kernel_initializer='he_normal',\n",
        "                   padding='same', name='initial_conv2D', strides=initial_strides,\n",
        "                   use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n",
        "\n",
        "        if subsample_initial_block:\n",
        "            x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5,\n",
        "                                   name='initial_bn')(x)\n",
        "            x = Activation('relu')(x)\n",
        "            x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "        # Add dense blocks\n",
        "        for block_idx in range(nb_dense_block - 1):\n",
        "            x, nb_filter = __dense_block(x, nb_layers[block_idx], nb_filter,\n",
        "                                         growth_rate, bottleneck=bottleneck,\n",
        "                                         dropout_rate=dropout_rate,\n",
        "                                         weight_decay=weight_decay,\n",
        "                                         block_prefix='dense_%i' % block_idx)\n",
        "            # add transition_block\n",
        "            x = __transition_block(x, nb_filter, compression=compression,\n",
        "                                   weight_decay=weight_decay,\n",
        "                                   block_prefix='tr_%i' % block_idx,\n",
        "                                   transition_pooling=transition_pooling)\n",
        "            nb_filter = int(nb_filter * compression)\n",
        "\n",
        "        # The last dense_block does not have a transition_block\n",
        "        x, nb_filter = __dense_block(x, final_nb_layer, nb_filter, growth_rate,\n",
        "                                     bottleneck=bottleneck, dropout_rate=dropout_rate,\n",
        "                                     weight_decay=weight_decay,\n",
        "                                     block_prefix='dense_%i' % (nb_dense_block - 1))\n",
        "\n",
        "        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5, name='final_bn')(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        if include_top:\n",
        "            if pooling == 'avg':\n",
        "                x = GlobalAveragePooling2D()(x)\n",
        "            elif pooling == 'max':\n",
        "                x = GlobalMaxPooling2D()(x)\n",
        "            x = Dense(nb_classes, activation=activation)(x)\n",
        "        else:\n",
        "            if pooling == 'avg':\n",
        "                x = GlobalAveragePooling2D()(x)\n",
        "            elif pooling == 'max':\n",
        "                x = GlobalMaxPooling2D()(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF5v2bpcwZCy",
        "outputId": "4bc44759-218a-48bf-c7b5-5726941d3487"
      },
      "source": [
        "model = __create_dense_net(num_classes, x_train.shape[1:], True)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 32, 32, 16)   448         input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 32, 32, 16)   64          conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 32, 32, 16)   0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 32, 32, 128)  18560       activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 32, 32, 128)  512         conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 32, 32, 128)  0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 128)  0           activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 32, 32, 128)  147584      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 32, 32, 128)  2176        activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_76 (Add)                    (None, 32, 32, 128)  0           conv2d_191[0][0]                 \n",
            "                                                                 conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 32, 32, 128)  512         add_76[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 32, 32, 128)  0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 32, 32, 128)  147584      activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 32, 32, 128)  512         conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 32, 32, 128)  0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 128)  0           activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 32, 32, 128)  147584      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_77 (Add)                    (None, 32, 32, 128)  0           conv2d_194[0][0]                 \n",
            "                                                                 add_76[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 32, 32, 128)  512         add_77[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 32, 32, 128)  0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 32, 32, 128)  147584      activation_169[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 32, 32, 128)  512         conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 32, 32, 128)  0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 128)  0           activation_170[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 32, 32, 128)  147584      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_78 (Add)                    (None, 32, 32, 128)  0           conv2d_196[0][0]                 \n",
            "                                                                 add_77[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 32, 32, 128)  512         add_78[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 32, 32, 128)  0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 32, 32, 128)  147584      activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 32, 32, 128)  512         conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 32, 32, 128)  0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 128)  0           activation_172[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 32, 32, 128)  147584      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_79 (Add)                    (None, 32, 32, 128)  0           conv2d_198[0][0]                 \n",
            "                                                                 add_78[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 32, 32, 128)  512         add_79[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 32, 32, 128)  0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 16, 16, 256)  295168      activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 16, 16, 256)  1024        conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 16, 16, 256)  0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 16, 16, 256)  0           activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 16, 16, 256)  590080      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 16, 16, 256)  33024       activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_80 (Add)                    (None, 16, 16, 256)  0           conv2d_200[0][0]                 \n",
            "                                                                 conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 16, 16, 256)  1024        add_80[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, 16, 16, 256)  0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 16, 16, 256)  590080      activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 16, 16, 256)  1024        conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, 16, 16, 256)  0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 16, 16, 256)  0           activation_176[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 16, 16, 256)  590080      dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_81 (Add)                    (None, 16, 16, 256)  0           conv2d_203[0][0]                 \n",
            "                                                                 add_80[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 16, 16, 256)  1024        add_81[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, 16, 16, 256)  0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 16, 16, 256)  590080      activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 16, 16, 256)  1024        conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, 16, 16, 256)  0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 16, 16, 256)  0           activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 16, 16, 256)  590080      dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_82 (Add)                    (None, 16, 16, 256)  0           conv2d_205[0][0]                 \n",
            "                                                                 add_81[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 16, 16, 256)  1024        add_82[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, 16, 16, 256)  0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 16, 16, 256)  590080      activation_179[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 16, 16, 256)  1024        conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 16, 16, 256)  0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 16, 16, 256)  0           activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 16, 16, 256)  590080      dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_83 (Add)                    (None, 16, 16, 256)  0           conv2d_207[0][0]                 \n",
            "                                                                 add_82[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 16, 16, 256)  1024        add_83[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 16, 16, 256)  0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 8, 8, 512)    1180160     activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 8, 8, 512)    2048        conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 8, 8, 512)    0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 8, 8, 512)    0           activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 8, 8, 512)    2359808     dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 8, 8, 512)    131584      activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_84 (Add)                    (None, 8, 8, 512)    0           conv2d_209[0][0]                 \n",
            "                                                                 conv2d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 8, 8, 512)    2048        add_84[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 8, 8, 512)    0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 8, 8, 512)    2359808     activation_183[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 8, 8, 512)    2048        conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 8, 8, 512)    0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 8, 8, 512)    0           activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 8, 8, 512)    2359808     dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_85 (Add)                    (None, 8, 8, 512)    0           conv2d_212[0][0]                 \n",
            "                                                                 add_84[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 8, 8, 512)    2048        add_85[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 8, 8, 512)    0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 8, 8, 512)    2359808     activation_185[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 8, 8, 512)    2048        conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 8, 8, 512)    0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 8, 8, 512)    0           activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 8, 8, 512)    2359808     dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_86 (Add)                    (None, 8, 8, 512)    0           conv2d_214[0][0]                 \n",
            "                                                                 add_85[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 8, 8, 512)    2048        add_86[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 8, 8, 512)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 8, 8, 512)    2359808     activation_187[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 8, 8, 512)    2048        conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 8, 8, 512)    0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 8, 8, 512)    0           activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 8, 8, 512)    2359808     dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_87 (Add)                    (None, 8, 8, 512)    0           conv2d_216[0][0]                 \n",
            "                                                                 add_86[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 8, 8, 512)    2048        add_87[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 8, 8, 512)    0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 512)          0           activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 10)           5130        global_average_pooling2d_4[0][0] \n",
            "==================================================================================================\n",
            "Total params: 23,377,290\n",
            "Trainable params: 23,362,922\n",
            "Non-trainable params: 14,368\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkDjl8exTTDZ"
      },
      "source": [
        "Define an optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTRuAT1FTTOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2518a943-180e-4e71-b101-921c2fc4a7be"
      },
      "source": [
        "opt = SGD(lr=0.1, decay=1e-6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmhjZqVhfV18"
      },
      "source": [
        "DEFINE A LEARNING RATE SCHEDULER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjGe1KAwfV7F"
      },
      "source": [
        "def scheduler(epoch):\n",
        "    if epoch < 25:\n",
        "        return .1\n",
        "    elif epoch < 50:\n",
        "        return 0.01\n",
        "    else:\n",
        "        return 0.001\n",
        "\n",
        "# Callbacks\n",
        "set_lr = LRS(scheduler)\n",
        "es = keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHhBHWFjTTYy"
      },
      "source": [
        "Compile the model, define loss and link the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W8KCPtcTTii"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBOKdV6MTTtA"
      },
      "source": [
        "Finally, train the model and evaluate over the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLPsSdVDTT37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae09fa2-c675-4dae-991b-907b39dedad8"
      },
      "source": [
        "history=model.fit(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                  steps_per_epoch=len(x_train) / batch_size, \n",
        "                  epochs=epochs,\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  callbacks=[set_lr, es],\n",
        "                  verbose=1)\n",
        "\n",
        "# Evaluate over test\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/75\n",
            " 28/500 [>.............................] - ETA: 3:06 - loss: 2.2770 - accuracy: 0.1584"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_oBDiZbbO0G"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4uLWmxCbaq9"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wlYhPH9bcz9"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}