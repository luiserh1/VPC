{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet_batch32.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORvwib-YSR-p"
      },
      "source": [
        "CIFAR10 - DenseNet solution. Adaptación de https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/applications/densenet.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YRXPjE-Sf1a"
      },
      "source": [
        "Install Keras just in case..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhsFJNHpSSKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4c21cc-e84b-4119-b415-4f0dacdbb81c"
      },
      "source": [
        "!pip3 install keras\n",
        "!pip3 install keras_applications"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.5.2)\n",
            "Collecting keras_applications\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications) (1.19.5)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications) (1.5.2)\n",
            "Installing collected packages: keras-applications\n",
            "Successfully installed keras-applications-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGX8RGwHSSTr"
      },
      "source": [
        "Imports..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8qeBuPiSSdc"
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "\n",
        "import warnings\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.pooling import MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Flatten, Input, Conv2D, Add, ZeroPadding2D, Dense, Dropout, Activation, Reshape, Conv2DTranspose, UpSampling2D, MaxPooling2D, AveragePooling2D, GlobalMaxPooling2D, concatenate\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.merge import add\n",
        "from keras import Model\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras.regularizers import l2\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from keras.applications.imagenet_utils import preprocess_input as _preprocess_input\n",
        "import keras.backend as K\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras import utils\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "import keras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M9uBGifSSqd"
      },
      "source": [
        "Define batch size, number of epochs and number of classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaWFWm8mSTBj"
      },
      "source": [
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 75"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuM-ZML22Cn9",
        "outputId": "2bed684b-8a94-4c79-96da-88bad74e469c"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "170508288/170498071 [==============================] - 4s 0us/step\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0CgF6jTejyx"
      },
      "source": [
        "## Data Augmentation with an ImageGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.3,\n",
        "    rotation_range=45,\n",
        "    vertical_flip=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdaMLtdhtd5j"
      },
      "source": [
        "Movidas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap_0BFfhRKEC"
      },
      "source": [
        "def __conv_block(ip, nb_filter, bottleneck=False, dropout_rate=None,\n",
        "                 weight_decay=1e-4, block_prefix=None):\n",
        "    '''\n",
        "    Adds a convolution layer (with batch normalization and relu),\n",
        "    and optionally a bottleneck layer.\n",
        "    # Arguments\n",
        "        ip: Input tensor\n",
        "        nb_filter: integer, the dimensionality of the output space\n",
        "            (i.e. the number output of filters in the convolution)\n",
        "        bottleneck: if True, adds a bottleneck convolution block\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay factor\n",
        "        block_prefix: str, for unique layer naming\n",
        "     # Input shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, channels, rows, cols)` if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, rows, cols, channels)` if data_format='channels_last'.\n",
        "    # Output shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n",
        "        `rows` and `cols` values might have changed due to stride.\n",
        "    # Returns\n",
        "        output tensor of block\n",
        "    '''\n",
        "    with K.name_scope('ConvBlock'):\n",
        "        concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5,\n",
        "                               name=name_or_none(block_prefix, '_bn'))(ip)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        if bottleneck:\n",
        "            inter_channel = nb_filter * 4\n",
        "\n",
        "            x = Conv2D(inter_channel, (1, 1), kernel_initializer='he_normal',\n",
        "                       padding='same', use_bias=False,\n",
        "                       kernel_regularizer=l2(weight_decay),\n",
        "                       name=name_or_none(block_prefix, '_bottleneck_conv2D'))(x)\n",
        "            x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5,\n",
        "                                   name=name_or_none(block_prefix, '_bottleneck_bn'))(x)\n",
        "            x = Activation('relu')(x)\n",
        "\n",
        "        x = Conv2D(nb_filter, (3, 3), kernel_initializer='he_normal', padding='same',\n",
        "                   use_bias=False, name=name_or_none(block_prefix, '_conv2D'))(x)\n",
        "        if dropout_rate:\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwkmg_n-Rssm"
      },
      "source": [
        "def __dense_block(x, nb_layers, nb_filter, growth_rate, bottleneck=False,\n",
        "                  dropout_rate=None, weight_decay=1e-4, grow_nb_filters=True,\n",
        "                  return_concat_list=False, block_prefix=None):\n",
        "    '''\n",
        "    Build a dense_block where the output of each conv_block is fed\n",
        "    to subsequent ones\n",
        "    # Arguments\n",
        "        x: input keras tensor\n",
        "        nb_layers: the number of conv_blocks to append to the model\n",
        "        nb_filter: integer, the dimensionality of the output space\n",
        "            (i.e. the number output of filters in the convolution)\n",
        "        growth_rate: growth rate of the dense block\n",
        "        bottleneck: if True, adds a bottleneck convolution block to\n",
        "            each conv_block\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay factor\n",
        "        grow_nb_filters: if True, allows number of filters to grow\n",
        "        return_concat_list: set to True to return the list of\n",
        "            feature maps along with the actual output\n",
        "        block_prefix: str, for block unique naming\n",
        "    # Return\n",
        "        If return_concat_list is True, returns a list of the output\n",
        "        keras tensor, the number of filters and a list of all the\n",
        "        dense blocks added to the keras tensor\n",
        "        If return_concat_list is False, returns a list of the output\n",
        "        keras tensor and the number of filters\n",
        "    '''\n",
        "    with K.name_scope('DenseBlock'):\n",
        "        concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "        x_list = [x]\n",
        "\n",
        "        for i in range(nb_layers):\n",
        "            cb = __conv_block(x, growth_rate, bottleneck, dropout_rate, weight_decay,\n",
        "                              block_prefix=name_or_none(block_prefix, '_%i' % i))\n",
        "            x_list.append(cb)\n",
        "\n",
        "            x = concatenate([x, cb], axis=concat_axis)\n",
        "\n",
        "            if grow_nb_filters:\n",
        "                nb_filter += growth_rate\n",
        "\n",
        "        if return_concat_list:\n",
        "            return x, nb_filter, x_list\n",
        "        else:\n",
        "            return x, nb_filter"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt9-YGpZRtoj"
      },
      "source": [
        "def __transition_block(ip, nb_filter, compression=1.0, weight_decay=1e-4,\n",
        "                       block_prefix=None, transition_pooling='max'):\n",
        "    '''\n",
        "    Adds a pointwise convolution layer (with batch normalization and relu),\n",
        "    and an average pooling layer. The number of output convolution filters\n",
        "    can be reduced by appropriately reducing the compression parameter.\n",
        "    # Arguments\n",
        "        ip: input keras tensor\n",
        "        nb_filter: integer, the dimensionality of the output space\n",
        "            (i.e. the number output of filters in the convolution)\n",
        "        compression: calculated as 1 - reduction. Reduces the number\n",
        "            of feature maps in the transition block.\n",
        "        weight_decay: weight decay factor\n",
        "        block_prefix: str, for block unique naming\n",
        "    # Input shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, channels, rows, cols)` if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, rows, cols, channels)` if data_format='channels_last'.\n",
        "    # Output shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, nb_filter * compression, rows / 2, cols / 2)`\n",
        "        if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, rows / 2, cols / 2, nb_filter * compression)`\n",
        "        if data_format='channels_last'.\n",
        "    # Returns\n",
        "        a keras tensor\n",
        "    '''\n",
        "    with K.name_scope('Transition'):\n",
        "        concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5,\n",
        "                               name=name_or_none(block_prefix, '_bn'))(ip)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Conv2D(int(nb_filter * compression), (1, 1), kernel_initializer='he_normal',\n",
        "                   padding='same', use_bias=False, kernel_regularizer=l2(weight_decay),\n",
        "                   name=name_or_none(block_prefix, '_conv2D'))(x)\n",
        "        if transition_pooling == 'avg':\n",
        "            x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "        elif transition_pooling == 'max':\n",
        "            x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pbEvFvxf5z_"
      },
      "source": [
        "def __transition_up_block(ip, nb_filters, type='deconv', weight_decay=1E-4,\n",
        "                          block_prefix=None):\n",
        "    '''Adds an upsampling block. Upsampling operation relies on the the type parameter.\n",
        "    # Arguments\n",
        "        ip: input keras tensor\n",
        "        nb_filters: integer, the dimensionality of the output space\n",
        "            (i.e. the number output of filters in the convolution)\n",
        "        type: can be 'upsampling', 'subpixel', 'deconv'. Determines\n",
        "            type of upsampling performed\n",
        "        weight_decay: weight decay factor\n",
        "        block_prefix: str, for block unique naming\n",
        "    # Input shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, channels, rows, cols)` if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, rows, cols, channels)` if data_format='channels_last'.\n",
        "    # Output shape\n",
        "        4D tensor with shape:\n",
        "        `(samples, nb_filter, rows * 2, cols * 2)` if data_format='channels_first'\n",
        "        or 4D tensor with shape:\n",
        "        `(samples, rows * 2, cols * 2, nb_filter)` if data_format='channels_last'.\n",
        "    # Returns\n",
        "        a keras tensor\n",
        "    '''\n",
        "    with K.name_scope('TransitionUp'):\n",
        "\n",
        "        if type == 'upsampling':\n",
        "            x = UpSampling2D(name=name_or_none(block_prefix, '_upsampling'))(ip)\n",
        "        elif type == 'subpixel':\n",
        "            x = Conv2D(nb_filters, (3, 3), activation='relu', padding='same',\n",
        "                       kernel_regularizer=l2(weight_decay), use_bias=False,\n",
        "                       kernel_initializer='he_normal',\n",
        "                       name=name_or_none(block_prefix, '_conv2D'))(ip)\n",
        "            x = SubPixelUpscaling(scale_factor=2,\n",
        "                                  name=name_or_none(block_prefix, '_subpixel'))(x)\n",
        "            x = Conv2D(nb_filters, (3, 3), activation='relu', padding='same',\n",
        "                       kernel_regularizer=l2(weight_decay), use_bias=False,\n",
        "                       kernel_initializer='he_normal',\n",
        "                       name=name_or_none(block_prefix, '_conv2D'))(x)\n",
        "        else:\n",
        "            x = Conv2DTranspose(nb_filters, (3, 3), activation='relu', padding='same',\n",
        "                                strides=(2, 2), kernel_initializer='he_normal',\n",
        "                                kernel_regularizer=l2(weight_decay),\n",
        "                                name=name_or_none(block_prefix, '_conv2DT'))(ip)\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9L_jJ0ff5_s"
      },
      "source": [
        "def __create_dense_net(nb_classes, img_input, include_top, depth=40, nb_dense_block=3,\n",
        "                       growth_rate=12, nb_filter=-1, nb_layers_per_block=-1,\n",
        "                       bottleneck=False, reduction=0.0, dropout_rate=None,\n",
        "                       weight_decay=1e-4, subsample_initial_block=False, pooling=None,\n",
        "                       activation='softmax', transition_pooling='avg'):\n",
        "    ''' Build the DenseNet model\n",
        "    # Arguments\n",
        "        nb_classes: number of classes\n",
        "        img_input: tuple of shape (channels, rows, columns) or (rows, columns, channels)\n",
        "        include_top: flag to include the final Dense layer\n",
        "        depth: number or layers\n",
        "        nb_dense_block: number of dense blocks to add to end (generally = 3)\n",
        "        growth_rate: number of filters to add per dense block\n",
        "        nb_filter: initial number of filters. Default -1 indicates initial number\n",
        "            of filters is 2 * growth_rate\n",
        "        nb_layers_per_block: number of layers in each dense block.\n",
        "                Can be a -1, positive integer or a list.\n",
        "                If -1, calculates nb_layer_per_block from the depth of the network.\n",
        "                If positive integer, a set number of layers per dense block.\n",
        "                If list, nb_layer is used as provided. Note that list size must\n",
        "                be (nb_dense_block + 1)\n",
        "        bottleneck: add bottleneck blocks\n",
        "        reduction: reduction factor of transition blocks. Note : reduction value is\n",
        "            inverted to compute compression\n",
        "        dropout_rate: dropout rate\n",
        "        weight_decay: weight decay rate\n",
        "        subsample_initial_block: Changes model type to suit different datasets.\n",
        "            Should be set to True for ImageNet, and False for CIFAR datasets.\n",
        "            When set to True, the initial convolution will be strided and\n",
        "            adds a MaxPooling2D before the initial dense block.\n",
        "        pooling: Optional pooling mode for feature extraction\n",
        "            when `include_top` is `False`.\n",
        "            - `None` means that the output of the model\n",
        "                will be the 4D tensor output of the\n",
        "                last convolutional layer.\n",
        "            - `avg` means that global average pooling\n",
        "                will be applied to the output of the\n",
        "                last convolutional layer, and thus\n",
        "                the output of the model will be a\n",
        "                2D tensor.\n",
        "            - `max` means that global max pooling will\n",
        "                be applied.\n",
        "        activation: Type of activation at the top layer. Can be one of 'softmax' or\n",
        "            'sigmoid'. Note that if sigmoid is used, classes must be 1.\n",
        "        transition_pooling: `avg` for avg pooling (default), `max` for max pooling,\n",
        "            None for no pooling during scale transition blocks. Please note that this\n",
        "            default differs from the DenseNetFCN paper in accordance with the DenseNet\n",
        "            paper.\n",
        "    # Returns\n",
        "        a keras tensor\n",
        "    # Raises\n",
        "        ValueError: in case of invalid argument for `reduction`\n",
        "            or `nb_dense_block`\n",
        "    '''\n",
        "    with K.name_scope('DenseNet'):\n",
        "        concat_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "        if reduction != 0.0:\n",
        "            if not (reduction <= 1.0 and reduction > 0.0):\n",
        "                raise ValueError('`reduction` value must lie between 0.0 and 1.0')\n",
        "\n",
        "        # layers in each dense block\n",
        "        if type(nb_layers_per_block) is list or type(nb_layers_per_block) is tuple:\n",
        "            nb_layers = list(nb_layers_per_block)  # Convert tuple to list\n",
        "\n",
        "            if len(nb_layers) != nb_dense_block:\n",
        "                raise ValueError('If `nb_dense_block` is a list, its length must match '\n",
        "                                 'the number of layers provided by `nb_layers`.')\n",
        "\n",
        "            final_nb_layer = nb_layers[-1]\n",
        "            nb_layers = nb_layers[:-1]\n",
        "        else:\n",
        "            if nb_layers_per_block == -1:\n",
        "                assert (depth - 4) % 3 == 0, ('Depth must be 3 N + 4 '\n",
        "                                              'if nb_layers_per_block == -1')\n",
        "                count = int((depth - 4) / 3)\n",
        "\n",
        "                if bottleneck:\n",
        "                    count = count // 2\n",
        "\n",
        "                nb_layers = [count for _ in range(nb_dense_block)]\n",
        "                final_nb_layer = count\n",
        "            else:\n",
        "                final_nb_layer = nb_layers_per_block\n",
        "                nb_layers = [nb_layers_per_block] * nb_dense_block\n",
        "\n",
        "        # compute initial nb_filter if -1, else accept users initial nb_filter\n",
        "        if nb_filter <= 0:\n",
        "            nb_filter = 2 * growth_rate\n",
        "\n",
        "        # compute compression factor\n",
        "        compression = 1.0 - reduction\n",
        "\n",
        "        # Initial convolution\n",
        "        if subsample_initial_block:\n",
        "            initial_kernel = (7, 7)\n",
        "            initial_strides = (2, 2)\n",
        "        else:\n",
        "            initial_kernel = (3, 3)\n",
        "            initial_strides = (1, 1)\n",
        "\n",
        "        x = Conv2D(nb_filter, initial_kernel, kernel_initializer='he_normal',\n",
        "                   padding='same', name='initial_conv2D', strides=initial_strides,\n",
        "                   use_bias=False, kernel_regularizer=l2(weight_decay))(img_input)\n",
        "\n",
        "        if subsample_initial_block:\n",
        "            x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5,\n",
        "                                   name='initial_bn')(x)\n",
        "            x = Activation('relu')(x)\n",
        "            x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "        # Add dense blocks\n",
        "        for block_idx in range(nb_dense_block - 1):\n",
        "            x, nb_filter = __dense_block(x, nb_layers[block_idx], nb_filter,\n",
        "                                         growth_rate, bottleneck=bottleneck,\n",
        "                                         dropout_rate=dropout_rate,\n",
        "                                         weight_decay=weight_decay,\n",
        "                                         block_prefix='dense_%i' % block_idx)\n",
        "            # add transition_block\n",
        "            x = __transition_block(x, nb_filter, compression=compression,\n",
        "                                   weight_decay=weight_decay,\n",
        "                                   block_prefix='tr_%i' % block_idx,\n",
        "                                   transition_pooling=transition_pooling)\n",
        "            nb_filter = int(nb_filter * compression)\n",
        "\n",
        "        # The last dense_block does not have a transition_block\n",
        "        x, nb_filter = __dense_block(x, final_nb_layer, nb_filter, growth_rate,\n",
        "                                     bottleneck=bottleneck, dropout_rate=dropout_rate,\n",
        "                                     weight_decay=weight_decay,\n",
        "                                     block_prefix='dense_%i' % (nb_dense_block - 1))\n",
        "\n",
        "        x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5, name='final_bn')(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        if include_top:\n",
        "            if pooling == 'avg':\n",
        "                x = GlobalAveragePooling2D()(x)\n",
        "            elif pooling == 'max':\n",
        "                x = GlobalMaxPooling2D()(x)\n",
        "            x = Dense(nb_classes, activation=activation)(x)\n",
        "        else:\n",
        "            if pooling == 'avg':\n",
        "                x = GlobalAveragePooling2D()(x)\n",
        "            elif pooling == 'max':\n",
        "                x = GlobalMaxPooling2D()(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCWB7VOgZ8SZ"
      },
      "source": [
        "def name_or_none(prefix, name):\n",
        "    return prefix + name if (prefix is not None and name is not None) else None"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF5v2bpcwZCy",
        "outputId": "c827c1e0-c8f5-40a8-f753-6e95aee3f943"
      },
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "input_shape = _obtain_input_shape(input_shape,\n",
        "                                  default_size=224,\n",
        "                                  min_size=32,\n",
        "                                  data_format=K.image_data_format(),\n",
        "                                  require_flatten=True,\n",
        "                                  weights=None)\n",
        "img_input = Input(shape=input_shape)\n",
        "x = __create_dense_net(num_classes, img_input, True)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(num_classes)(x)\n",
        "x = Activation('softmax')(x)\n",
        "\n",
        "model = Model(get_source_inputs(img_input), x, name='densenet')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"densenet\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "initial_conv2D (Conv2D)         (None, 32, 32, 24)   648         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_0_bn (BatchNormalizatio (None, 32, 32, 24)   96          initial_conv2D[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 24)   0           dense_0_0_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_0_conv2D (Conv2D)       (None, 32, 32, 12)   2592        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 36)   0           initial_conv2D[0][0]             \n",
            "                                                                 dense_0_0_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_1_bn (BatchNormalizatio (None, 32, 32, 36)   144         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 36)   0           dense_0_1_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_1_conv2D (Conv2D)       (None, 32, 32, 12)   3888        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 48)   0           concatenate[0][0]                \n",
            "                                                                 dense_0_1_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_2_bn (BatchNormalizatio (None, 32, 32, 48)   192         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 48)   0           dense_0_2_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_2_conv2D (Conv2D)       (None, 32, 32, 12)   5184        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 60)   0           concatenate_1[0][0]              \n",
            "                                                                 dense_0_2_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_3_bn (BatchNormalizatio (None, 32, 32, 60)   240         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 60)   0           dense_0_3_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_3_conv2D (Conv2D)       (None, 32, 32, 12)   6480        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 72)   0           concatenate_2[0][0]              \n",
            "                                                                 dense_0_3_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_4_bn (BatchNormalizatio (None, 32, 32, 72)   288         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 72)   0           dense_0_4_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_4_conv2D (Conv2D)       (None, 32, 32, 12)   7776        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 84)   0           concatenate_3[0][0]              \n",
            "                                                                 dense_0_4_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_5_bn (BatchNormalizatio (None, 32, 32, 84)   336         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 84)   0           dense_0_5_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_5_conv2D (Conv2D)       (None, 32, 32, 12)   9072        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 96)   0           concatenate_4[0][0]              \n",
            "                                                                 dense_0_5_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_6_bn (BatchNormalizatio (None, 32, 32, 96)   384         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 96)   0           dense_0_6_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_6_conv2D (Conv2D)       (None, 32, 32, 12)   10368       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 108)  0           concatenate_5[0][0]              \n",
            "                                                                 dense_0_6_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_7_bn (BatchNormalizatio (None, 32, 32, 108)  432         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 108)  0           dense_0_7_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_7_conv2D (Conv2D)       (None, 32, 32, 12)   11664       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 120)  0           concatenate_6[0][0]              \n",
            "                                                                 dense_0_7_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_8_bn (BatchNormalizatio (None, 32, 32, 120)  480         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 120)  0           dense_0_8_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_8_conv2D (Conv2D)       (None, 32, 32, 12)   12960       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 132)  0           concatenate_7[0][0]              \n",
            "                                                                 dense_0_8_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_9_bn (BatchNormalizatio (None, 32, 32, 132)  528         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 132)  0           dense_0_9_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_9_conv2D (Conv2D)       (None, 32, 32, 12)   14256       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 144)  0           concatenate_8[0][0]              \n",
            "                                                                 dense_0_9_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_10_bn (BatchNormalizati (None, 32, 32, 144)  576         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 144)  0           dense_0_10_bn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_10_conv2D (Conv2D)      (None, 32, 32, 12)   15552       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 156)  0           concatenate_9[0][0]              \n",
            "                                                                 dense_0_10_conv2D[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_11_bn (BatchNormalizati (None, 32, 32, 156)  624         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 156)  0           dense_0_11_bn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_0_11_conv2D (Conv2D)      (None, 32, 32, 12)   16848       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 168)  0           concatenate_10[0][0]             \n",
            "                                                                 dense_0_11_conv2D[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tr_0_bn (BatchNormalization)    (None, 32, 32, 168)  672         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 168)  0           tr_0_bn[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tr_0_conv2D (Conv2D)            (None, 32, 32, 168)  28224       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 168)  0           tr_0_conv2D[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_0_bn (BatchNormalizatio (None, 16, 16, 168)  672         average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 168)  0           dense_1_0_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_0_conv2D (Conv2D)       (None, 16, 16, 12)   18144       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 16, 16, 180)  0           average_pooling2d[0][0]          \n",
            "                                                                 dense_1_0_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_1_bn (BatchNormalizatio (None, 16, 16, 180)  720         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 180)  0           dense_1_1_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_1_conv2D (Conv2D)       (None, 16, 16, 12)   19440       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 192)  0           concatenate_12[0][0]             \n",
            "                                                                 dense_1_1_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_2_bn (BatchNormalizatio (None, 16, 16, 192)  768         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 192)  0           dense_1_2_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_2_conv2D (Conv2D)       (None, 16, 16, 12)   20736       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 204)  0           concatenate_13[0][0]             \n",
            "                                                                 dense_1_2_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_3_bn (BatchNormalizatio (None, 16, 16, 204)  816         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 204)  0           dense_1_3_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_3_conv2D (Conv2D)       (None, 16, 16, 12)   22032       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 216)  0           concatenate_14[0][0]             \n",
            "                                                                 dense_1_3_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_4_bn (BatchNormalizatio (None, 16, 16, 216)  864         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 216)  0           dense_1_4_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_4_conv2D (Conv2D)       (None, 16, 16, 12)   23328       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 228)  0           concatenate_15[0][0]             \n",
            "                                                                 dense_1_4_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_5_bn (BatchNormalizatio (None, 16, 16, 228)  912         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 228)  0           dense_1_5_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_5_conv2D (Conv2D)       (None, 16, 16, 12)   24624       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 240)  0           concatenate_16[0][0]             \n",
            "                                                                 dense_1_5_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_6_bn (BatchNormalizatio (None, 16, 16, 240)  960         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 240)  0           dense_1_6_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_6_conv2D (Conv2D)       (None, 16, 16, 12)   25920       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 252)  0           concatenate_17[0][0]             \n",
            "                                                                 dense_1_6_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_7_bn (BatchNormalizatio (None, 16, 16, 252)  1008        concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 252)  0           dense_1_7_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_7_conv2D (Conv2D)       (None, 16, 16, 12)   27216       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 264)  0           concatenate_18[0][0]             \n",
            "                                                                 dense_1_7_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_8_bn (BatchNormalizatio (None, 16, 16, 264)  1056        concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 264)  0           dense_1_8_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_8_conv2D (Conv2D)       (None, 16, 16, 12)   28512       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 276)  0           concatenate_19[0][0]             \n",
            "                                                                 dense_1_8_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_9_bn (BatchNormalizatio (None, 16, 16, 276)  1104        concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 276)  0           dense_1_9_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_9_conv2D (Conv2D)       (None, 16, 16, 12)   29808       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 288)  0           concatenate_20[0][0]             \n",
            "                                                                 dense_1_9_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_10_bn (BatchNormalizati (None, 16, 16, 288)  1152        concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 288)  0           dense_1_10_bn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_10_conv2D (Conv2D)      (None, 16, 16, 12)   31104       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 300)  0           concatenate_21[0][0]             \n",
            "                                                                 dense_1_10_conv2D[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_11_bn (BatchNormalizati (None, 16, 16, 300)  1200        concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 300)  0           dense_1_11_bn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_11_conv2D (Conv2D)      (None, 16, 16, 12)   32400       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 312)  0           concatenate_22[0][0]             \n",
            "                                                                 dense_1_11_conv2D[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tr_1_bn (BatchNormalization)    (None, 16, 16, 312)  1248        concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 312)  0           tr_1_bn[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tr_1_conv2D (Conv2D)            (None, 16, 16, 312)  97344       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 8, 8, 312)    0           tr_1_conv2D[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_0_bn (BatchNormalizatio (None, 8, 8, 312)    1248        average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 312)    0           dense_2_0_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_0_conv2D (Conv2D)       (None, 8, 8, 12)     33696       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 8, 8, 324)    0           average_pooling2d_1[0][0]        \n",
            "                                                                 dense_2_0_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_1_bn (BatchNormalizatio (None, 8, 8, 324)    1296        concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 324)    0           dense_2_1_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_1_conv2D (Conv2D)       (None, 8, 8, 12)     34992       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 336)    0           concatenate_24[0][0]             \n",
            "                                                                 dense_2_1_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_2_bn (BatchNormalizatio (None, 8, 8, 336)    1344        concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 336)    0           dense_2_2_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_2_conv2D (Conv2D)       (None, 8, 8, 12)     36288       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 348)    0           concatenate_25[0][0]             \n",
            "                                                                 dense_2_2_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_3_bn (BatchNormalizatio (None, 8, 8, 348)    1392        concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 348)    0           dense_2_3_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_3_conv2D (Conv2D)       (None, 8, 8, 12)     37584       activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 360)    0           concatenate_26[0][0]             \n",
            "                                                                 dense_2_3_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_4_bn (BatchNormalizatio (None, 8, 8, 360)    1440        concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 360)    0           dense_2_4_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_4_conv2D (Conv2D)       (None, 8, 8, 12)     38880       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 372)    0           concatenate_27[0][0]             \n",
            "                                                                 dense_2_4_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_5_bn (BatchNormalizatio (None, 8, 8, 372)    1488        concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 372)    0           dense_2_5_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_5_conv2D (Conv2D)       (None, 8, 8, 12)     40176       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 384)    0           concatenate_28[0][0]             \n",
            "                                                                 dense_2_5_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_6_bn (BatchNormalizatio (None, 8, 8, 384)    1536        concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 384)    0           dense_2_6_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_6_conv2D (Conv2D)       (None, 8, 8, 12)     41472       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 396)    0           concatenate_29[0][0]             \n",
            "                                                                 dense_2_6_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_7_bn (BatchNormalizatio (None, 8, 8, 396)    1584        concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 396)    0           dense_2_7_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_7_conv2D (Conv2D)       (None, 8, 8, 12)     42768       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 408)    0           concatenate_30[0][0]             \n",
            "                                                                 dense_2_7_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_8_bn (BatchNormalizatio (None, 8, 8, 408)    1632        concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 408)    0           dense_2_8_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_8_conv2D (Conv2D)       (None, 8, 8, 12)     44064       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 420)    0           concatenate_31[0][0]             \n",
            "                                                                 dense_2_8_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_9_bn (BatchNormalizatio (None, 8, 8, 420)    1680        concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 420)    0           dense_2_9_bn[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_9_conv2D (Conv2D)       (None, 8, 8, 12)     45360       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 432)    0           concatenate_32[0][0]             \n",
            "                                                                 dense_2_9_conv2D[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_10_bn (BatchNormalizati (None, 8, 8, 432)    1728        concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 432)    0           dense_2_10_bn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_10_conv2D (Conv2D)      (None, 8, 8, 12)     46656       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 444)    0           concatenate_33[0][0]             \n",
            "                                                                 dense_2_10_conv2D[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_11_bn (BatchNormalizati (None, 8, 8, 444)    1776        concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 444)    0           dense_2_11_bn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_11_conv2D (Conv2D)      (None, 8, 8, 12)     47952       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 456)    0           concatenate_34[0][0]             \n",
            "                                                                 dense_2_11_conv2D[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "final_bn (BatchNormalization)   (None, 8, 8, 456)    1824        concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 456)    0           final_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 8, 8, 10)     4570        activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 640)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           6410        flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 10)           0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,084,428\n",
            "Trainable params: 1,065,708\n",
            "Non-trainable params: 18,720\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkDjl8exTTDZ"
      },
      "source": [
        "Define an optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTRuAT1FTTOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79bfc9f8-8151-46fb-dde9-5bb2b30c9bec"
      },
      "source": [
        "opt = SGD(lr=0.1, decay=1e-6)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmhjZqVhfV18"
      },
      "source": [
        "DEFINE A LEARNING RATE SCHEDULER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjGe1KAwfV7F"
      },
      "source": [
        "def scheduler(epoch):\n",
        "    if epoch < 25:\n",
        "        return .1\n",
        "    elif epoch < 50:\n",
        "        return 0.01\n",
        "    else:\n",
        "        return 0.001\n",
        "\n",
        "# Callbacks\n",
        "set_lr = LRS(scheduler)\n",
        "es = keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHhBHWFjTTYy"
      },
      "source": [
        "Compile the model, define loss and link the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W8KCPtcTTii"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBOKdV6MTTtA"
      },
      "source": [
        "Finally, train the model and evaluate over the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLPsSdVDTT37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb328eb1-bfdf-4602-b53d-ffe0a1f34fd2"
      },
      "source": [
        "history=model.fit(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                  steps_per_epoch=len(x_train) / batch_size, \n",
        "                  epochs=epochs,\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  callbacks=[set_lr, es],\n",
        "                  verbose=1)\n",
        "\n",
        "# Evaluate over test\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/75\n",
            "1562/1562 [==============================] - 167s 71ms/step - loss: 1.8981 - accuracy: 0.3392 - val_loss: 1.5948 - val_accuracy: 0.4569\n",
            "Epoch 2/75\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 1.5878 - accuracy: 0.4621 - val_loss: 1.5791 - val_accuracy: 0.4856\n",
            "Epoch 3/75\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 1.4102 - accuracy: 0.5267 - val_loss: 1.2625 - val_accuracy: 0.5845\n",
            "Epoch 4/75\n",
            "1562/1562 [==============================] - 107s 68ms/step - loss: 1.2857 - accuracy: 0.5740 - val_loss: 1.3136 - val_accuracy: 0.5731\n",
            "Epoch 5/75\n",
            "1562/1562 [==============================] - 107s 68ms/step - loss: 1.1916 - accuracy: 0.6079 - val_loss: 1.1148 - val_accuracy: 0.6391\n",
            "Epoch 6/75\n",
            "1562/1562 [==============================] - 112s 72ms/step - loss: 1.1190 - accuracy: 0.6362 - val_loss: 1.0134 - val_accuracy: 0.6828\n",
            "Epoch 7/75\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 1.0606 - accuracy: 0.6588 - val_loss: 1.0224 - val_accuracy: 0.6800\n",
            "Epoch 8/75\n",
            "1562/1562 [==============================] - 107s 68ms/step - loss: 1.0071 - accuracy: 0.6800 - val_loss: 0.9579 - val_accuracy: 0.6974\n",
            "Epoch 9/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.9551 - accuracy: 0.6956 - val_loss: 0.8987 - val_accuracy: 0.7196\n",
            "Epoch 10/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.9064 - accuracy: 0.7137 - val_loss: 1.0174 - val_accuracy: 0.6879\n",
            "Epoch 11/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.8741 - accuracy: 0.7213 - val_loss: 0.9590 - val_accuracy: 0.7146\n",
            "Epoch 12/75\n",
            "1562/1562 [==============================] - 107s 68ms/step - loss: 0.8529 - accuracy: 0.7317 - val_loss: 0.9022 - val_accuracy: 0.7242\n",
            "Epoch 13/75\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.8086 - accuracy: 0.7465 - val_loss: 0.9845 - val_accuracy: 0.7170\n",
            "Epoch 14/75\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.7732 - accuracy: 0.7557 - val_loss: 0.8719 - val_accuracy: 0.7414\n",
            "Epoch 15/75\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.7705 - accuracy: 0.7598 - val_loss: 0.8410 - val_accuracy: 0.7569\n",
            "Epoch 16/75\n",
            "1562/1562 [==============================] - 107s 68ms/step - loss: 0.7441 - accuracy: 0.7664 - val_loss: 0.7127 - val_accuracy: 0.7881\n",
            "Epoch 17/75\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.7161 - accuracy: 0.7770 - val_loss: 0.6569 - val_accuracy: 0.8049\n",
            "Epoch 18/75\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.6918 - accuracy: 0.7831 - val_loss: 0.6845 - val_accuracy: 0.7946\n",
            "Epoch 19/75\n",
            "1562/1562 [==============================] - 107s 68ms/step - loss: 0.6847 - accuracy: 0.7862 - val_loss: 0.7484 - val_accuracy: 0.7835\n",
            "Epoch 20/75\n",
            "1562/1562 [==============================] - 107s 69ms/step - loss: 0.6591 - accuracy: 0.7984 - val_loss: 0.5868 - val_accuracy: 0.8278\n",
            "Epoch 21/75\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.6459 - accuracy: 0.8013 - val_loss: 0.7754 - val_accuracy: 0.7721\n",
            "Epoch 22/75\n",
            "1562/1562 [==============================] - 106s 68ms/step - loss: 0.6300 - accuracy: 0.8075 - val_loss: 0.6826 - val_accuracy: 0.7939\n",
            "Epoch 23/75\n",
            "1562/1562 [==============================] - 107s 68ms/step - loss: 0.6160 - accuracy: 0.8111 - val_loss: 0.5542 - val_accuracy: 0.8355\n",
            "Epoch 24/75\n",
            "1562/1562 [==============================] - 107s 68ms/step - loss: 0.6031 - accuracy: 0.8144 - val_loss: 0.5668 - val_accuracy: 0.8333\n",
            "Epoch 25/75\n",
            "1562/1562 [==============================] - 107s 68ms/step - loss: 0.5848 - accuracy: 0.8205 - val_loss: 0.6086 - val_accuracy: 0.8250\n",
            "Epoch 26/75\n",
            "1562/1562 [==============================] - 112s 72ms/step - loss: 0.5203 - accuracy: 0.8446 - val_loss: 0.4750 - val_accuracy: 0.8631\n",
            "Epoch 27/75\n",
            "1562/1562 [==============================] - 109s 70ms/step - loss: 0.4766 - accuracy: 0.8605 - val_loss: 0.4490 - val_accuracy: 0.8738\n",
            "Epoch 28/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.4650 - accuracy: 0.8623 - val_loss: 0.4590 - val_accuracy: 0.8690\n",
            "Epoch 29/75\n",
            "1562/1562 [==============================] - 107s 69ms/step - loss: 0.4554 - accuracy: 0.8657 - val_loss: 0.4551 - val_accuracy: 0.8693\n",
            "Epoch 30/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.4471 - accuracy: 0.8676 - val_loss: 0.4432 - val_accuracy: 0.8741\n",
            "Epoch 31/75\n",
            "1562/1562 [==============================] - 107s 69ms/step - loss: 0.4466 - accuracy: 0.8687 - val_loss: 0.4461 - val_accuracy: 0.8718\n",
            "Epoch 32/75\n",
            "1562/1562 [==============================] - 107s 69ms/step - loss: 0.4378 - accuracy: 0.8713 - val_loss: 0.4707 - val_accuracy: 0.8676\n",
            "Epoch 33/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.4400 - accuracy: 0.8702 - val_loss: 0.4370 - val_accuracy: 0.8750\n",
            "Epoch 34/75\n",
            "1562/1562 [==============================] - 112s 72ms/step - loss: 0.4332 - accuracy: 0.8737 - val_loss: 0.4472 - val_accuracy: 0.8725\n",
            "Epoch 35/75\n",
            "1562/1562 [==============================] - 107s 69ms/step - loss: 0.4283 - accuracy: 0.8733 - val_loss: 0.4408 - val_accuracy: 0.8741\n",
            "Epoch 36/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.4263 - accuracy: 0.8757 - val_loss: 0.4432 - val_accuracy: 0.8763\n",
            "Epoch 37/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.4240 - accuracy: 0.8745 - val_loss: 0.4610 - val_accuracy: 0.8703\n",
            "Epoch 38/75\n",
            "1562/1562 [==============================] - 107s 69ms/step - loss: 0.4138 - accuracy: 0.8794 - val_loss: 0.4689 - val_accuracy: 0.8689\n",
            "Epoch 39/75\n",
            "1562/1562 [==============================] - 112s 72ms/step - loss: 0.4205 - accuracy: 0.8770 - val_loss: 0.4482 - val_accuracy: 0.8742\n",
            "Epoch 40/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.4254 - accuracy: 0.8734 - val_loss: 0.4441 - val_accuracy: 0.8758\n",
            "Epoch 41/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.4096 - accuracy: 0.8796 - val_loss: 0.4415 - val_accuracy: 0.8756\n",
            "Epoch 42/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.4067 - accuracy: 0.8825 - val_loss: 0.4332 - val_accuracy: 0.8774\n",
            "Epoch 43/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.4062 - accuracy: 0.8817 - val_loss: 0.4304 - val_accuracy: 0.8785\n",
            "Epoch 44/75\n",
            "1562/1562 [==============================] - 107s 69ms/step - loss: 0.4044 - accuracy: 0.8788 - val_loss: 0.4428 - val_accuracy: 0.8732\n",
            "Epoch 45/75\n",
            "1562/1562 [==============================] - 107s 69ms/step - loss: 0.3948 - accuracy: 0.8855 - val_loss: 0.4234 - val_accuracy: 0.8819\n",
            "Epoch 46/75\n",
            "1562/1562 [==============================] - 107s 69ms/step - loss: 0.3979 - accuracy: 0.8833 - val_loss: 0.4328 - val_accuracy: 0.8782\n",
            "Epoch 47/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3950 - accuracy: 0.8842 - val_loss: 0.4164 - val_accuracy: 0.8818\n",
            "Epoch 48/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3895 - accuracy: 0.8871 - val_loss: 0.4399 - val_accuracy: 0.8771\n",
            "Epoch 49/75\n",
            "1562/1562 [==============================] - 107s 69ms/step - loss: 0.3958 - accuracy: 0.8828 - val_loss: 0.4426 - val_accuracy: 0.8758\n",
            "Epoch 50/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3899 - accuracy: 0.8861 - val_loss: 0.4288 - val_accuracy: 0.8780\n",
            "Epoch 51/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3816 - accuracy: 0.8860 - val_loss: 0.4162 - val_accuracy: 0.8837\n",
            "Epoch 52/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3817 - accuracy: 0.8886 - val_loss: 0.4168 - val_accuracy: 0.8836\n",
            "Epoch 53/75\n",
            "1562/1562 [==============================] - 107s 69ms/step - loss: 0.3708 - accuracy: 0.8931 - val_loss: 0.4156 - val_accuracy: 0.8835\n",
            "Epoch 54/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3785 - accuracy: 0.8888 - val_loss: 0.4114 - val_accuracy: 0.8846\n",
            "Epoch 55/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3726 - accuracy: 0.8909 - val_loss: 0.4125 - val_accuracy: 0.8843\n",
            "Epoch 56/75\n",
            "1562/1562 [==============================] - 107s 69ms/step - loss: 0.3756 - accuracy: 0.8920 - val_loss: 0.4190 - val_accuracy: 0.8821\n",
            "Epoch 57/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3797 - accuracy: 0.8871 - val_loss: 0.4126 - val_accuracy: 0.8839\n",
            "Epoch 58/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3753 - accuracy: 0.8892 - val_loss: 0.4138 - val_accuracy: 0.8831\n",
            "Epoch 59/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3734 - accuracy: 0.8917 - val_loss: 0.4172 - val_accuracy: 0.8829\n",
            "Epoch 60/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3740 - accuracy: 0.8912 - val_loss: 0.4113 - val_accuracy: 0.8839\n",
            "Epoch 61/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3655 - accuracy: 0.8944 - val_loss: 0.4133 - val_accuracy: 0.8837\n",
            "Epoch 62/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3665 - accuracy: 0.8901 - val_loss: 0.4122 - val_accuracy: 0.8838\n",
            "Epoch 63/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3684 - accuracy: 0.8945 - val_loss: 0.4085 - val_accuracy: 0.8846\n",
            "Epoch 64/75\n",
            "1562/1562 [==============================] - 108s 69ms/step - loss: 0.3709 - accuracy: 0.8902 - val_loss: 0.4135 - val_accuracy: 0.8846\n",
            "Test loss: 0.4134795069694519\n",
            "Test accuracy: 0.8845999836921692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_oBDiZbbO0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f8a784-6f03-44ab-c64f-2b982a4dfd34"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4uLWmxCbaq9"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wlYhPH9bcz9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "430b7ea5-423a-4582-d74f-c96439cb3468"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}