{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WideResnet_K8_NoPreTrained.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORvwib-YSR-p"
      },
      "source": [
        "CIFAR10 - WideResnet solution. AdaptaciÃ³n de https://www.kaggle.com/zjaume/resnet-cifar10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YRXPjE-Sf1a"
      },
      "source": [
        "Install Keras just in case..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhsFJNHpSSKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adda23f5-b6ba-4f59-ea61-7beb481f0824"
      },
      "source": [
        "!pip3 install keras\n",
        "!pip3 install keras_applications"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.5.2)\n",
            "Requirement already satisfied: keras_applications in /usr/local/lib/python3.7/dist-packages (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras_applications) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras_applications) (1.19.5)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras_applications) (1.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGX8RGwHSSTr"
      },
      "source": [
        "Imports..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8qeBuPiSSdc"
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "\n",
        "import warnings\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.pooling import MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Input, Conv2D, Add, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization as  BN\n",
        "from keras.layers.merge import add\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.engine.topology import get_source_inputs\n",
        "import keras.backend as K\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from keras import utils\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "import keras"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M9uBGifSSqd"
      },
      "source": [
        "Define batch size, number of epochs and number of classes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaWFWm8mSTBj"
      },
      "source": [
        "batch_size = 100\n",
        "num_classes = 10\n",
        "epochs = 75"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuM-ZML22Cn9",
        "outputId": "00e38689-94cd-4ab8-ae20-3ea8f3d735c5"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0CgF6jTejyx"
      },
      "source": [
        "## Data Augmentation with an ImageGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.3,\n",
        "    rotation_range=45,\n",
        "    vertical_flip=False)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdaMLtdhtd5j"
      },
      "source": [
        "Movidas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap_0BFfhRKEC"
      },
      "source": [
        "def bn_act(x, activation='relu'):\n",
        "    l = BN()(x)\n",
        "    if activation=='prelu':\n",
        "        l = PReLU()(l)\n",
        "    else:\n",
        "        l = Activation('relu')(l)\n",
        "    return l"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwkmg_n-Rssm"
      },
      "source": [
        "def res_block(convs, identity=True, k=1):\n",
        "    def inner(x):\n",
        "        if not identity:\n",
        "            strides = (2,2)\n",
        "        else:\n",
        "            strides = (1,1)\n",
        "            \n",
        "        act = bn_act(x)\n",
        "        l = Conv2D(convs*k, 3, strides=strides, padding='same', kernel_initializer='he_normal')(act)\n",
        "        \n",
        "        l = bn_act(l)\n",
        "        l = Dropout(0.5)(l)\n",
        "        l = Conv2D(convs*k, 3, strides=(1,1), padding='same', kernel_initializer='he_normal')(l)\n",
        "        \n",
        "        if not identity or x.shape[3]!=convs*k:\n",
        "            shortcut = Conv2D(convs*k, 1, strides=strides, padding='same', kernel_initializer='he_normal')(act)\n",
        "            l = Add()([l,shortcut])\n",
        "        else:\n",
        "            l = Add()([l,x])\n",
        "        return l\n",
        "    return inner"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt9-YGpZRtoj"
      },
      "source": [
        "def wide_resnet(input_shape, k = 8):\n",
        "    inpt = Input(shape = input_shape)\n",
        "\n",
        "    # stage 1\n",
        "    x = Conv2D(16, 3, strides=(1,1), padding='same', kernel_initializer='he_normal')(inpt)\n",
        "    #x = bn_act(x)\n",
        "    #x = MaxPooling2D(pool_size=(3,3),strides=2)(x)\n",
        "    \n",
        "    n=28 # depth or total number of layers\n",
        "    N = (n-4)//6\n",
        "    # stage 2\n",
        "    for i in range(N):\n",
        "        x = res_block(16,k=k)(x)\n",
        "\n",
        "    # stage 3\n",
        "    x = res_block(32,False,k=k)(x)\n",
        "    for i in range(1,N):\n",
        "        x = res_block(32,k=k)(x)\n",
        "\n",
        "    # stage 4\n",
        "    x = res_block(64,False,k=k)(x)\n",
        "    for i in range(1,N):\n",
        "        x = res_block(64,k=k)(x)\n",
        "    \n",
        "    x = bn_act(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    outpt = Dense(num_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inpt,outpt)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF5v2bpcwZCy",
        "outputId": "4bc44759-218a-48bf-c7b5-5726941d3487"
      },
      "source": [
        "model = wide_resnet(x_train.shape[1:], 8)\n",
        "model.summary()"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_189 (Conv2D)             (None, 32, 32, 16)   448         input_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 32, 32, 16)   64          conv2d_189[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 32, 32, 16)   0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_190 (Conv2D)             (None, 32, 32, 128)  18560       activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 32, 32, 128)  512         conv2d_190[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 32, 32, 128)  0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 128)  0           activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_191 (Conv2D)             (None, 32, 32, 128)  147584      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_192 (Conv2D)             (None, 32, 32, 128)  2176        activation_165[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_76 (Add)                    (None, 32, 32, 128)  0           conv2d_191[0][0]                 \n",
            "                                                                 conv2d_192[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 32, 32, 128)  512         add_76[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 32, 32, 128)  0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_193 (Conv2D)             (None, 32, 32, 128)  147584      activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 32, 32, 128)  512         conv2d_193[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 32, 32, 128)  0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 128)  0           activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_194 (Conv2D)             (None, 32, 32, 128)  147584      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_77 (Add)                    (None, 32, 32, 128)  0           conv2d_194[0][0]                 \n",
            "                                                                 add_76[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 32, 32, 128)  512         add_77[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 32, 32, 128)  0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_195 (Conv2D)             (None, 32, 32, 128)  147584      activation_169[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 32, 32, 128)  512         conv2d_195[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 32, 32, 128)  0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 128)  0           activation_170[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_196 (Conv2D)             (None, 32, 32, 128)  147584      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_78 (Add)                    (None, 32, 32, 128)  0           conv2d_196[0][0]                 \n",
            "                                                                 add_77[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 32, 32, 128)  512         add_78[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 32, 32, 128)  0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_197 (Conv2D)             (None, 32, 32, 128)  147584      activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 32, 32, 128)  512         conv2d_197[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 32, 32, 128)  0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 128)  0           activation_172[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_198 (Conv2D)             (None, 32, 32, 128)  147584      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_79 (Add)                    (None, 32, 32, 128)  0           conv2d_198[0][0]                 \n",
            "                                                                 add_78[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 32, 32, 128)  512         add_79[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 32, 32, 128)  0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_199 (Conv2D)             (None, 16, 16, 256)  295168      activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 16, 16, 256)  1024        conv2d_199[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 16, 16, 256)  0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 16, 16, 256)  0           activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_200 (Conv2D)             (None, 16, 16, 256)  590080      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_201 (Conv2D)             (None, 16, 16, 256)  33024       activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_80 (Add)                    (None, 16, 16, 256)  0           conv2d_200[0][0]                 \n",
            "                                                                 conv2d_201[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 16, 16, 256)  1024        add_80[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, 16, 16, 256)  0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_202 (Conv2D)             (None, 16, 16, 256)  590080      activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 16, 16, 256)  1024        conv2d_202[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, 16, 16, 256)  0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 16, 16, 256)  0           activation_176[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_203 (Conv2D)             (None, 16, 16, 256)  590080      dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_81 (Add)                    (None, 16, 16, 256)  0           conv2d_203[0][0]                 \n",
            "                                                                 add_80[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 16, 16, 256)  1024        add_81[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, 16, 16, 256)  0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_204 (Conv2D)             (None, 16, 16, 256)  590080      activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 16, 16, 256)  1024        conv2d_204[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, 16, 16, 256)  0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 16, 16, 256)  0           activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_205 (Conv2D)             (None, 16, 16, 256)  590080      dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_82 (Add)                    (None, 16, 16, 256)  0           conv2d_205[0][0]                 \n",
            "                                                                 add_81[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 16, 16, 256)  1024        add_82[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, 16, 16, 256)  0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_206 (Conv2D)             (None, 16, 16, 256)  590080      activation_179[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 16, 16, 256)  1024        conv2d_206[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 16, 16, 256)  0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 16, 16, 256)  0           activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_207 (Conv2D)             (None, 16, 16, 256)  590080      dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_83 (Add)                    (None, 16, 16, 256)  0           conv2d_207[0][0]                 \n",
            "                                                                 add_82[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 16, 16, 256)  1024        add_83[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 16, 16, 256)  0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_208 (Conv2D)             (None, 8, 8, 512)    1180160     activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 8, 8, 512)    2048        conv2d_208[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 8, 8, 512)    0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 8, 8, 512)    0           activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_209 (Conv2D)             (None, 8, 8, 512)    2359808     dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_210 (Conv2D)             (None, 8, 8, 512)    131584      activation_181[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_84 (Add)                    (None, 8, 8, 512)    0           conv2d_209[0][0]                 \n",
            "                                                                 conv2d_210[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 8, 8, 512)    2048        add_84[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 8, 8, 512)    0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_211 (Conv2D)             (None, 8, 8, 512)    2359808     activation_183[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 8, 8, 512)    2048        conv2d_211[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 8, 8, 512)    0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 8, 8, 512)    0           activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_212 (Conv2D)             (None, 8, 8, 512)    2359808     dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_85 (Add)                    (None, 8, 8, 512)    0           conv2d_212[0][0]                 \n",
            "                                                                 add_84[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 8, 8, 512)    2048        add_85[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 8, 8, 512)    0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_213 (Conv2D)             (None, 8, 8, 512)    2359808     activation_185[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 8, 8, 512)    2048        conv2d_213[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 8, 8, 512)    0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 8, 8, 512)    0           activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_214 (Conv2D)             (None, 8, 8, 512)    2359808     dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_86 (Add)                    (None, 8, 8, 512)    0           conv2d_214[0][0]                 \n",
            "                                                                 add_85[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 8, 8, 512)    2048        add_86[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 8, 8, 512)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_215 (Conv2D)             (None, 8, 8, 512)    2359808     activation_187[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_188 (BatchN (None, 8, 8, 512)    2048        conv2d_215[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_188 (Activation)     (None, 8, 8, 512)    0           batch_normalization_188[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 8, 8, 512)    0           activation_188[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_216 (Conv2D)             (None, 8, 8, 512)    2359808     dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_87 (Add)                    (None, 8, 8, 512)    0           conv2d_216[0][0]                 \n",
            "                                                                 add_86[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_189 (BatchN (None, 8, 8, 512)    2048        add_87[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_189 (Activation)     (None, 8, 8, 512)    0           batch_normalization_189[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 512)          0           activation_189[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 10)           5130        global_average_pooling2d_4[0][0] \n",
            "==================================================================================================\n",
            "Total params: 23,377,290\n",
            "Trainable params: 23,362,922\n",
            "Non-trainable params: 14,368\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkDjl8exTTDZ"
      },
      "source": [
        "Define an optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTRuAT1FTTOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2518a943-180e-4e71-b101-921c2fc4a7be"
      },
      "source": [
        "opt = SGD(lr=0.1, decay=1e-6)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmhjZqVhfV18"
      },
      "source": [
        "DEFINE A LEARNING RATE SCHEDULER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjGe1KAwfV7F"
      },
      "source": [
        "def scheduler(epoch):\n",
        "    if epoch < 25:\n",
        "        return .1\n",
        "    elif epoch < 50:\n",
        "        return 0.01\n",
        "    else:\n",
        "        return 0.001\n",
        "\n",
        "# Callbacks\n",
        "set_lr = LRS(scheduler)\n",
        "es = keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHhBHWFjTTYy"
      },
      "source": [
        "Compile the model, define loss and link the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W8KCPtcTTii"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBOKdV6MTTtA"
      },
      "source": [
        "Finally, train the model and evaluate over the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLPsSdVDTT37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae09fa2-c675-4dae-991b-907b39dedad8"
      },
      "source": [
        "history=model.fit(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
        "                  steps_per_epoch=len(x_train) / batch_size, \n",
        "                  epochs=epochs,\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  callbacks=[set_lr, es],\n",
        "                  verbose=1)\n",
        "\n",
        "# Evaluate over test\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/75\n",
            "500/500 [==============================] - 209s 405ms/step - loss: 1.9909 - accuracy: 0.2524 - val_loss: 1.7618 - val_accuracy: 0.4010\n",
            "Epoch 2/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 1.6411 - accuracy: 0.3936 - val_loss: 2.0816 - val_accuracy: 0.3531\n",
            "Epoch 3/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 1.4920 - accuracy: 0.4565 - val_loss: 2.3830 - val_accuracy: 0.3812\n",
            "Epoch 4/75\n",
            "500/500 [==============================] - 198s 396ms/step - loss: 1.3715 - accuracy: 0.5025 - val_loss: 1.3522 - val_accuracy: 0.5488\n",
            "Epoch 5/75\n",
            "500/500 [==============================] - 198s 397ms/step - loss: 1.2954 - accuracy: 0.5321 - val_loss: 1.6495 - val_accuracy: 0.4966\n",
            "Epoch 6/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 1.2187 - accuracy: 0.5597 - val_loss: 1.5749 - val_accuracy: 0.5263\n",
            "Epoch 7/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 1.1605 - accuracy: 0.5845 - val_loss: 1.0712 - val_accuracy: 0.6391\n",
            "Epoch 8/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 1.1035 - accuracy: 0.6059 - val_loss: 1.3055 - val_accuracy: 0.5946\n",
            "Epoch 9/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 1.0548 - accuracy: 0.6254 - val_loss: 1.0764 - val_accuracy: 0.6524\n",
            "Epoch 10/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 1.0117 - accuracy: 0.6394 - val_loss: 1.1316 - val_accuracy: 0.6265\n",
            "Epoch 11/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.9720 - accuracy: 0.6559 - val_loss: 1.1903 - val_accuracy: 0.6224\n",
            "Epoch 12/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.9360 - accuracy: 0.6729 - val_loss: 0.8813 - val_accuracy: 0.7087\n",
            "Epoch 13/75\n",
            "500/500 [==============================] - 199s 397ms/step - loss: 0.8999 - accuracy: 0.6825 - val_loss: 0.8939 - val_accuracy: 0.7163\n",
            "Epoch 14/75\n",
            "500/500 [==============================] - 198s 397ms/step - loss: 0.8718 - accuracy: 0.6929 - val_loss: 0.7517 - val_accuracy: 0.7567\n",
            "Epoch 15/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.8381 - accuracy: 0.7059 - val_loss: 0.8314 - val_accuracy: 0.7321\n",
            "Epoch 16/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.8215 - accuracy: 0.7128 - val_loss: 0.8170 - val_accuracy: 0.7412\n",
            "Epoch 17/75\n",
            "500/500 [==============================] - 198s 397ms/step - loss: 0.7937 - accuracy: 0.7205 - val_loss: 0.6529 - val_accuracy: 0.7804\n",
            "Epoch 18/75\n",
            "500/500 [==============================] - 198s 397ms/step - loss: 0.7697 - accuracy: 0.7301 - val_loss: 0.7627 - val_accuracy: 0.7564\n",
            "Epoch 19/75\n",
            "500/500 [==============================] - 198s 397ms/step - loss: 0.7544 - accuracy: 0.7351 - val_loss: 0.7603 - val_accuracy: 0.7570\n",
            "Epoch 20/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.7272 - accuracy: 0.7441 - val_loss: 0.7974 - val_accuracy: 0.7471\n",
            "Epoch 21/75\n",
            "500/500 [==============================] - 199s 397ms/step - loss: 0.7159 - accuracy: 0.7491 - val_loss: 0.7370 - val_accuracy: 0.7757\n",
            "Epoch 22/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.6945 - accuracy: 0.7578 - val_loss: 0.5956 - val_accuracy: 0.8010\n",
            "Epoch 23/75\n",
            "500/500 [==============================] - 199s 397ms/step - loss: 0.6850 - accuracy: 0.7601 - val_loss: 0.6719 - val_accuracy: 0.7868\n",
            "Epoch 24/75\n",
            "500/500 [==============================] - 199s 397ms/step - loss: 0.6593 - accuracy: 0.7694 - val_loss: 0.6200 - val_accuracy: 0.8000\n",
            "Epoch 25/75\n",
            "500/500 [==============================] - 198s 397ms/step - loss: 0.6509 - accuracy: 0.7721 - val_loss: 0.7576 - val_accuracy: 0.7739\n",
            "Epoch 26/75\n",
            "500/500 [==============================] - 199s 397ms/step - loss: 0.5961 - accuracy: 0.7903 - val_loss: 0.5136 - val_accuracy: 0.8347\n",
            "Epoch 27/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.5665 - accuracy: 0.8034 - val_loss: 0.5105 - val_accuracy: 0.8359\n",
            "Epoch 28/75\n",
            "500/500 [==============================] - 199s 397ms/step - loss: 0.5697 - accuracy: 0.8022 - val_loss: 0.5069 - val_accuracy: 0.8393\n",
            "Epoch 29/75\n",
            "500/500 [==============================] - 198s 397ms/step - loss: 0.5610 - accuracy: 0.8050 - val_loss: 0.5214 - val_accuracy: 0.8374\n",
            "Epoch 30/75\n",
            "500/500 [==============================] - 198s 396ms/step - loss: 0.5568 - accuracy: 0.8039 - val_loss: 0.5045 - val_accuracy: 0.8419\n",
            "Epoch 31/75\n",
            "500/500 [==============================] - 198s 397ms/step - loss: 0.5515 - accuracy: 0.8056 - val_loss: 0.4936 - val_accuracy: 0.8423\n",
            "Epoch 32/75\n",
            "500/500 [==============================] - 198s 396ms/step - loss: 0.5512 - accuracy: 0.8092 - val_loss: 0.5003 - val_accuracy: 0.8417\n",
            "Epoch 33/75\n",
            "500/500 [==============================] - 198s 396ms/step - loss: 0.5425 - accuracy: 0.8125 - val_loss: 0.5103 - val_accuracy: 0.8389\n",
            "Epoch 34/75\n",
            "500/500 [==============================] - 198s 396ms/step - loss: 0.5414 - accuracy: 0.8111 - val_loss: 0.5013 - val_accuracy: 0.8425\n",
            "Epoch 35/75\n",
            "500/500 [==============================] - 198s 397ms/step - loss: 0.5389 - accuracy: 0.8120 - val_loss: 0.4866 - val_accuracy: 0.8453\n",
            "Epoch 36/75\n",
            "500/500 [==============================] - 198s 397ms/step - loss: 0.5413 - accuracy: 0.8109 - val_loss: 0.4777 - val_accuracy: 0.8480\n",
            "Epoch 37/75\n",
            "500/500 [==============================] - 198s 396ms/step - loss: 0.5406 - accuracy: 0.8117 - val_loss: 0.4795 - val_accuracy: 0.8454\n",
            "Epoch 38/75\n",
            "500/500 [==============================] - 198s 397ms/step - loss: 0.5367 - accuracy: 0.8145 - val_loss: 0.4849 - val_accuracy: 0.8474\n",
            "Epoch 39/75\n",
            "500/500 [==============================] - 199s 397ms/step - loss: 0.5398 - accuracy: 0.8143 - val_loss: 0.5044 - val_accuracy: 0.8423\n",
            "Epoch 40/75\n",
            "500/500 [==============================] - 199s 397ms/step - loss: 0.5383 - accuracy: 0.8121 - val_loss: 0.4879 - val_accuracy: 0.8453\n",
            "Epoch 41/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.5351 - accuracy: 0.8105 - val_loss: 0.4831 - val_accuracy: 0.8480\n",
            "Epoch 42/75\n",
            "500/500 [==============================] - 207s 414ms/step - loss: 0.5226 - accuracy: 0.8176 - val_loss: 0.4900 - val_accuracy: 0.8480\n",
            "Epoch 43/75\n",
            "500/500 [==============================] - 199s 397ms/step - loss: 0.5356 - accuracy: 0.8122 - val_loss: 0.5042 - val_accuracy: 0.8435\n",
            "Epoch 44/75\n",
            "500/500 [==============================] - 199s 397ms/step - loss: 0.5249 - accuracy: 0.8142 - val_loss: 0.4788 - val_accuracy: 0.8495\n",
            "Epoch 45/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.5226 - accuracy: 0.8162 - val_loss: 0.4935 - val_accuracy: 0.8454\n",
            "Epoch 46/75\n",
            "500/500 [==============================] - 198s 396ms/step - loss: 0.5207 - accuracy: 0.8190 - val_loss: 0.4856 - val_accuracy: 0.8460\n",
            "Epoch 47/75\n",
            "500/500 [==============================] - 198s 397ms/step - loss: 0.5248 - accuracy: 0.8166 - val_loss: 0.4722 - val_accuracy: 0.8530\n",
            "Epoch 48/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.5185 - accuracy: 0.8183 - val_loss: 0.4781 - val_accuracy: 0.8519\n",
            "Epoch 49/75\n",
            "500/500 [==============================] - 199s 397ms/step - loss: 0.5153 - accuracy: 0.8206 - val_loss: 0.4781 - val_accuracy: 0.8497\n",
            "Epoch 50/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.5098 - accuracy: 0.8217 - val_loss: 0.4791 - val_accuracy: 0.8488\n",
            "Epoch 51/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.5105 - accuracy: 0.8230 - val_loss: 0.4684 - val_accuracy: 0.8531\n",
            "Epoch 52/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.5162 - accuracy: 0.8189 - val_loss: 0.4637 - val_accuracy: 0.8545\n",
            "Epoch 53/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.5081 - accuracy: 0.8220 - val_loss: 0.4636 - val_accuracy: 0.8547\n",
            "Epoch 54/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.5013 - accuracy: 0.8263 - val_loss: 0.4650 - val_accuracy: 0.8540\n",
            "Epoch 55/75\n",
            "500/500 [==============================] - 199s 398ms/step - loss: 0.5072 - accuracy: 0.8205 - val_loss: 0.4669 - val_accuracy: 0.8539\n",
            "Epoch 56/75\n",
            "500/500 [==============================] - 199s 397ms/step - loss: 0.4948 - accuracy: 0.8278 - val_loss: 0.4639 - val_accuracy: 0.8551\n",
            "Test loss: 0.4639243185520172\n",
            "Test accuracy: 0.8550999760627747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_oBDiZbbO0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "122c08c8-f45e-4727-bc35-74a3e313fb06"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4uLWmxCbaq9"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wlYhPH9bcz9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "0892b78a-aa0d-44ad-dec3-ae9c060dcac3"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}